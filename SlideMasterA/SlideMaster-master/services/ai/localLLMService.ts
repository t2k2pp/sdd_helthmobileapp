// =================================================================
// Local LLM Service - Ollama, LM Studio, Llama.cppå¯¾å¿œ
// OpenAIäº’æ›APIã‚’ä½¿ç”¨ã—ãŸçµ±ä¸€ã‚µãƒ¼ãƒ“ã‚¹
// =================================================================

import { UnifiedAIService, TextGenerationOptions, ImageGenerationOptions, SlideImageOptions, EnhancedGenerationOptions, AIServiceError } from './unifiedAIService';
import { getUserSettings, LocalLLMConfig } from '../storageService';

// ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
const DEFAULT_ENDPOINTS = {
    ollama: 'http://localhost:11434',
    lmstudio: 'http://localhost:1234',
    llamacpp: 'http://localhost:8080',
};

const DEFAULT_TIMEOUT = 120000; // 2åˆ†
const DEFAULT_MAX_TOKENS = 4096;

// OpenAIäº’æ›APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹
interface OpenAICompatibleResponse {
    id: string;
    object: string;
    created: number;
    model: string;
    choices: Array<{
        index: number;
        message?: {
            role: string;
            content: string;
        };
        text?: string;
        finish_reason: string;
    }>;
    usage?: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
}

// Ollamaå›ºæœ‰ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹
interface OllamaGenerateResponse {
    model: string;
    created_at: string;
    response: string;
    done: boolean;
    context?: number[];
    total_duration?: number;
    load_duration?: number;
    prompt_eval_count?: number;
    prompt_eval_duration?: number;
    eval_count?: number;
    eval_duration?: number;
}

// OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™å…±é€šé–¢æ•°
async function callOpenAICompatibleAPI(
    endpoint: string,
    model: string,
    prompt: string,
    options?: TextGenerationOptions,
    timeout: number = DEFAULT_TIMEOUT
): Promise<string> {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
        const response = await fetch(`${endpoint}/v1/chat/completions`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                model: model,
                messages: [
                    ...(options?.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
                    { role: 'user', content: prompt }
                ],
                temperature: options?.temperature ?? 0.7,
                max_tokens: options?.maxTokens ?? DEFAULT_MAX_TOKENS,
                stream: false,
            }),
            signal: controller.signal,
        });

        clearTimeout(timeoutId);

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`API Error (${response.status}): ${errorText}`);
        }

        const data: OpenAICompatibleResponse = await response.json();

        if (data.choices && data.choices.length > 0) {
            const choice = data.choices[0];
            return choice.message?.content || choice.text || '';
        }

        throw new Error('Empty response from API');
    } catch (error) {
        clearTimeout(timeoutId);
        if (error instanceof Error && error.name === 'AbortError') {
            throw new Error(`Request timed out after ${timeout / 1000} seconds`);
        }
        throw error;
    }
}

// Ollamaå›ºæœ‰ã®APIã‚’å‘¼ã³å‡ºã™é–¢æ•°ï¼ˆ/api/generateï¼‰
async function callOllamaGenerateAPI(
    endpoint: string,
    model: string,
    prompt: string,
    options?: TextGenerationOptions,
    timeout: number = DEFAULT_TIMEOUT
): Promise<string> {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
        // ã¾ãšã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        const fullPrompt = options?.systemPrompt
            ? `${options.systemPrompt}\n\nUser: ${prompt}\n\nAssistant:`
            : prompt;

        const response = await fetch(`${endpoint}/api/generate`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                model: model,
                prompt: fullPrompt,
                stream: false,
                options: {
                    temperature: options?.temperature ?? 0.7,
                    num_predict: options?.maxTokens ?? DEFAULT_MAX_TOKENS,
                },
            }),
            signal: controller.signal,
        });

        clearTimeout(timeoutId);

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`Ollama API Error (${response.status}): ${errorText}`);
        }

        const data: OllamaGenerateResponse = await response.json();
        return data.response || '';
    } catch (error) {
        clearTimeout(timeoutId);
        if (error instanceof Error && error.name === 'AbortError') {
            throw new Error(`Request timed out after ${timeout / 1000} seconds`);
        }
        throw error;
    }
}

// ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹: ãƒ­ãƒ¼ã‚«ãƒ«LLMå…±é€šæ©Ÿèƒ½
abstract class BaseLocalLLMService implements UnifiedAIService {
    protected config: LocalLLMConfig;
    protected providerName: string;

    constructor(providerName: string, defaultEndpoint: string) {
        this.providerName = providerName;
        const settings = getUserSettings();
        const providerConfig = settings.providerAuth?.[providerName as keyof typeof settings.providerAuth] as LocalLLMConfig | undefined;

        this.config = {
            endpoint: providerConfig?.endpoint || defaultEndpoint,
            modelName: providerConfig?.modelName || '',
            timeout: providerConfig?.timeout || DEFAULT_TIMEOUT,
            maxTokens: providerConfig?.maxTokens || DEFAULT_MAX_TOKENS,
        };

        if (!this.config.endpoint) {
            throw new AIServiceError(
                `${providerName} ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“`,
                providerName,
                'CONFIG_MISSING'
            );
        }
    }

    abstract generateText(prompt: string, options?: TextGenerationOptions): Promise<string>;

    async generateImage(prompt: string, options?: ImageGenerationOptions): Promise<string> {
        throw new AIServiceError(
            `${this.providerName}ã¯ç”»åƒç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“`,
            this.providerName,
            'UNSUPPORTED_OPERATION'
        );
    }

    async generateSlideContent(
        topic: string,
        slideCount?: number,
        enhancedOptions?: EnhancedGenerationOptions
    ): Promise<string> {
        if (enhancedOptions?.enhancedPrompt) {
            console.log(`ğŸ¯ ${this.providerName}: Using enhanced prompt!`);
            return await this.generateText(enhancedOptions.enhancedPrompt, {
                systemPrompt: 'ã‚ãªãŸã¯å„ªç§€ãªãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã§ã™ã€‚æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚',
                temperature: 0.7,
            });
        }
        throw new AIServiceError(
            'å¼·åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¿…è¦ã§ã™',
            this.providerName,
            'MISSING_ENHANCED_PROMPT'
        );
    }

    async generateSlideImage(prompt: string, options?: SlideImageOptions): Promise<string> {
        throw new AIServiceError(
            `${this.providerName}ã¯ç”»åƒç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚Stable Diffusionã¾ãŸã¯ComfyUIã‚’ç”»åƒç”Ÿæˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã—ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚`,
            this.providerName,
            'UNSUPPORTED_OPERATION'
        );
    }

    async analyzeVideo(videoData: string, prompt?: string): Promise<string> {
        throw new AIServiceError(
            `${this.providerName}ã¯å‹•ç”»åˆ†æã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚Azure OpenAIã¾ãŸã¯Geminiã‚’å‹•ç”»åˆ†æãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã—ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚`,
            this.providerName,
            'UNSUPPORTED_OPERATION'
        );
    }

    getMaxTokens(safetyMargin: number = 0.9): number {
        return Math.floor((this.config.maxTokens || DEFAULT_MAX_TOKENS) * safetyMargin);
    }

    getModelInfo(): { service: string; model: string; limits: any } | null {
        return {
            service: this.providerName,
            model: this.config.modelName || 'unknown',
            limits: {
                maxTokens: this.config.maxTokens || DEFAULT_MAX_TOKENS,
                timeout: this.config.timeout || DEFAULT_TIMEOUT,
            },
        };
    }

    // EnhancedAIServiceã®ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
    async generateVideoSlides(request: any): Promise<any> {
        throw new AIServiceError(
            `${this.providerName}ã¯å‹•ç”»ã‹ã‚‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚Azure OpenAIã¾ãŸã¯Geminiã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚`,
            this.providerName,
            'UNSUPPORTED_OPERATION'
        );
    }

    async generateSlideImages(slides: any[], theme: string, imageSettings: any): Promise<{ [slideId: string]: string }> {
        throw new AIServiceError(
            `${this.providerName}ã¯ã‚¹ãƒ©ã‚¤ãƒ‰ç”»åƒç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚Stable Diffusionã¾ãŸã¯ComfyUIã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚`,
            this.providerName,
            'UNSUPPORTED_OPERATION'
        );
    }

    getProviderInfo(): { name: string; version: string; capabilities: string[] } {
        return {
            name: this.providerName,
            version: '1.0.0',
            capabilities: ['text-generation', 'slide-content-generation'],
        };
    }

    abstract testConnection(): Promise<boolean>;
}

// Ollamaå®Ÿè£…ã‚¯ãƒ©ã‚¹
export class OllamaUnifiedService extends BaseLocalLLMService {
    constructor() {
        super('ollama', DEFAULT_ENDPOINTS.ollama);
    }

    async generateText(prompt: string, options?: TextGenerationOptions): Promise<string> {
        try {
            if (!this.config.modelName) {
                throw new AIServiceError(
                    'Ollamaãƒ¢ãƒ‡ãƒ«åãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šç”»é¢ã§ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚',
                    'ollama',
                    'MODEL_NOT_CONFIGURED'
                );
            }

            // ã¾ãšOpenAIäº’æ›APIã‚’è©¦ã™
            try {
                return await callOpenAICompatibleAPI(
                    this.config.endpoint,
                    this.config.modelName,
                    prompt,
                    options,
                    this.config.timeout
                );
            } catch (openAIError) {
                // OpenAIäº’æ›APIãŒå¤±æ•—ã—ãŸå ´åˆã€Ollamaå›ºæœ‰APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                console.log('OpenAIäº’æ›APIãŒå¤±æ•—ã€Ollamaå›ºæœ‰APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯');
                return await callOllamaGenerateAPI(
                    this.config.endpoint,
                    this.config.modelName,
                    prompt,
                    options,
                    this.config.timeout
                );
            }
        } catch (error) {
            throw new AIServiceError(
                error instanceof Error ? error.message : 'ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'ollama',
                'TEXT_GENERATION_ERROR'
            );
        }
    }

    async testConnection(): Promise<boolean> {
        try {
            const response = await fetch(`${this.config.endpoint}/api/tags`, {
                method: 'GET',
                headers: { 'Content-Type': 'application/json' },
            });
            return response.ok;
        } catch (error) {
            console.error('Ollama connection test failed:', error);
            return false;
        }
    }
}

// LM Studioå®Ÿè£…ã‚¯ãƒ©ã‚¹
export class LMStudioUnifiedService extends BaseLocalLLMService {
    constructor() {
        super('lmstudio', DEFAULT_ENDPOINTS.lmstudio);
    }

    async generateText(prompt: string, options?: TextGenerationOptions): Promise<string> {
        try {
            // LM Studioã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«åã¯ç©ºã§ã‚‚OK
            const modelName = this.config.modelName || 'local-model';

            return await callOpenAICompatibleAPI(
                this.config.endpoint,
                modelName,
                prompt,
                options,
                this.config.timeout
            );
        } catch (error) {
            throw new AIServiceError(
                error instanceof Error ? error.message : 'ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'lmstudio',
                'TEXT_GENERATION_ERROR'
            );
        }
    }

    async testConnection(): Promise<boolean> {
        try {
            const response = await fetch(`${this.config.endpoint}/v1/models`, {
                method: 'GET',
                headers: { 'Content-Type': 'application/json' },
            });
            return response.ok;
        } catch (error) {
            console.error('LM Studio connection test failed:', error);
            return false;
        }
    }
}

// Llama.cppå®Ÿè£…ã‚¯ãƒ©ã‚¹
export class LlamaCppUnifiedService extends BaseLocalLLMService {
    constructor() {
        super('llamacpp', DEFAULT_ENDPOINTS.llamacpp);
    }

    async generateText(prompt: string, options?: TextGenerationOptions): Promise<string> {
        try {
            // Llama.cppã¯OpenAIäº’æ›APIã‚’ã‚µãƒãƒ¼ãƒˆ
            const modelName = this.config.modelName || 'llama';

            return await callOpenAICompatibleAPI(
                this.config.endpoint,
                modelName,
                prompt,
                options,
                this.config.timeout
            );
        } catch (error) {
            throw new AIServiceError(
                error instanceof Error ? error.message : 'ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'llamacpp',
                'TEXT_GENERATION_ERROR'
            );
        }
    }

    async testConnection(): Promise<boolean> {
        try {
            // Llama.cppã®healthã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            const response = await fetch(`${this.config.endpoint}/health`, {
                method: 'GET',
            });
            return response.ok;
        } catch (error) {
            // healthãŒç„¡ã„å ´åˆã€modelsã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦ã™
            try {
                const response = await fetch(`${this.config.endpoint}/v1/models`, {
                    method: 'GET',
                });
                return response.ok;
            } catch {
                console.error('Llama.cpp connection test failed:', error);
                return false;
            }
        }
    }
}

// ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
export function createLocalLLMService(providerType: 'ollama' | 'lmstudio' | 'llamacpp'): UnifiedAIService {
    switch (providerType) {
        case 'ollama':
            return new OllamaUnifiedService();
        case 'lmstudio':
            return new LMStudioUnifiedService();
        case 'llamacpp':
            return new LlamaCppUnifiedService();
        default:
            throw new AIServiceError(
                `æœªçŸ¥ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ${providerType}`,
                'localLLM',
                'UNKNOWN_PROVIDER'
            );
    }
}
